---
title: 3. 线性模型
date: 2021-06-12
categories: 
  - MachineLearning-周志华
tags:
  - readnotes
  - ml
---

## 3.1 基本形式

给定由 $\Large d$ 个属性描述的示例 $\Large \boldsymbol{x}= \{x_1;x_2,...,x_d\}$ ，则**线性模型（linear model）**试图学得一个通过属性的线性组合来进行预测的函数。即：
$$
\Large
f(\boldsymbol{x}) = w_1x_1 + w_2x_2+...+w_dx_d + b
$$
用向量的形式则可以写为：
$$
\Large
f(\boldsymbol{x}) = \boldsymbol{w}^T\boldsymbol{x} + b
$$
其中 $\Large \boldsymbol{w} = (w_1;w_2;...;w_d)$。

线性模型：

- 形式简单，易于建模；
- 是很多强大的非线性模型（unlinear model）的基础，可以简单通过非线性变化得到非线性模型；
- 有很好的可解释性，可以解释不同属性的影响大小；

## 3.2 线性回归

先考虑只有一个输入属性的情况，数据集 $\Large D = \{(x_i , y_i) \}^{m}_{i=1}\quad, x_i \in \Bbb{R}$  ，对于属性是离散的情况：

- 如果属性可以排序，如”身高“、”体重“，则可以将其连续化；
- 如果不可排序，如”颜色“，”性别“，则根据其取值的可能数，转化为 $\Large k$ 维向量；如 $\Large (0,0,1),(1,0,0),(0,1,0)$ ；如果强行对无序属性连续化，会对后续处理造成误导；

线性回归试图学得：
$$
\Large
f(x_i) = wx_i + b，使得 f(x_i) \simeq y_i
$$
目的是求解 $\Large w$ 和 $\Large b$ 使得上一章中介绍过的均方误差（2.4）最小，即：
$$
\Large
\begin{align}\begin{split}
(w^*, b^*) &= arg\;min_{w,b} 
\sum^m_{i=1}(f(x_i) - y_i)^2 \\
&= arg \; min_{w, b}
\sum^m_{i=1}(y_i - wx_i -b)^2
\end{split}\end{align}
$$
可以看出，均方误差就是离散的点到直线的欧氏距离之和。基于均方误差求解模型的方法的几何意义就是试图找出一条直线，使得离散的样本点到这条直线的欧氏距离之和最小，这种求解的方法我们在高中就学过，称为”**最小二乘法**“。

当样本确定时，我们将均方误差写为更通用的关于 $\Large w$ 和 $\Large b$ 的函数形式：
$$
\Large
E(w, b) = \sum^m_{i=1}(y_i - wx_i -b)^2
$$
分别对 $\Large w$ 和 $\Large b$ 求偏导，令偏导等于 $\Large 0$，即可解得 $\Large w$ 和 $\Large b$ 的最优解。
$$
\Large
\cfrac{\partial E_{(w, b)}}{\partial w} = 
2(w\sum^m_{i=1}x_i^2-\sum^m_{i=1}(y_i-b)x_i) = 0
$$

$$
\Large
\cfrac{\partial E_{(w, b)}}{\partial b} = 
2(mb-\sum^m_{i=1}(y_i-wx_i)) = 0
$$

得：
$$
\Large
w= \cfrac
{\sum^m_{i=1}y_i(x_i - \bar x)}
{\sum^m_{i=1}x_i^2 - \cfrac{1}{m}(\sum^m_{i=1}x_i)^2},其中\bar x =\cfrac1m\sum^m_{i=1}x_i
$$

$$
\Large
b= \cfrac1m\sum^m_{i=1}(y_i-wx_i)
$$


> 关于最小二乘法的本质和计算参考：[最小二乘法的本质是什么？](https://www.zhihu.com/question/37031188)

---

推广到具有$\Large d$ 个属性得数据集，此时试图学得：
$$
\Large
f(\boldsymbol{x}_i)=\boldsymbol w^T\boldsymbol x_i + b\quad ,使得 f(\boldsymbol x_i) \simeq y_i
$$
同样可以使用最小二乘法来计算 $\Large \boldsymbol w$ 和 $\Large b$ ，设 $\Large \hat {\boldsymbol{w}}=(\boldsymbol w;b)$，将数据集 $\Large D$ 视为一个 $\Large m \times (d + 1)$ 大小得矩阵 $\Large \bf{X}$ ，其中得每行代表一个示例，最后一列恒定为 $\Large 1$，即：
$$
\Large
\bf{X} = 
\begin{pmatrix}
x_{11} & x_{12} & \cdots & x_{1d} & 1 \\
x_{21} & x_{22} & \cdots & x_{2d} & 1 \\
\vdots  & \vdots & \ddots & \vdots & \vdots \\
x_{m1} & x_{m2} & \cdots & x_{md} & 1
\end{pmatrix}
= 
\begin{pmatrix}
\boldsymbol x_1^T & 1 \\
\boldsymbol x_2^T & 1 \\
\vdots & \vdots \\
\boldsymbol x_m^T & 1 \\
\end{pmatrix}
$$
再令标记 $\Large \boldsymbol y = (y_1;y_2;...;y_m)$ ，则类似于式(4)有：
$$
\Large
\hat{\boldsymbol{w}}^* = arg\;\;min_{\hat{\boldsymbol{w}}}(\boldsymbol{y} - \bf{X}\hat{\boldsymbol{w}})^{\rm T}(\boldsymbol{y}-\bf{X}\hat{\boldsymbol{w}})
$$
同理，求导后可得：
$$
\Large
\cfrac{\partial E_{\hat{\boldsymbol{w}}}}{\partial \hat{\boldsymbol{w}}} =
2\;\bf{X}^{\rm T}(\bf{X}\hat{\boldsymbol{w}} - y) 
= 2\bf{X}^{\rm T}\bf{X}\hat{\boldsymbol{w}} - 2\bf{X}^{\rm T}\boldsymbol y
$$
当 $\Large \bf{X}^{\rm T}\bf{X}$ 为正定矩阵时（或$\Large \bf{X}$满秩时），在一介导为零的点去极小值，此时的 $\Large  \hat{ \boldsymbol w}$ 就是我们找的使均方误差最小的值。

> 更多关于正定矩阵/满秩矩阵和导数、极值之间的关系可以参考 [正定矩阵的几何意义](http://www.fuzihao.org/blog/2014/03/14/%E6%AD%A3%E5%AE%9A%E7%9F%A9%E9%98%B5%E5%9C%A8%E5%A4%9A%E5%85%83%E5%87%BD%E6%95%B0%E5%BE%AE%E7%A7%AF%E5%88%86%E4%B8%AD%E7%9A%84%E5%87%A0%E4%BD%95%E6%84%8F%E4%B9%89/) 和 [怎么解释「正定矩阵」](https://www.zhihu.com/question/304499772/answer/552481133)。

此时：
$$
\Large
\hat{\boldsymbol{w}}^* = (\bf{X}^{\rm T}\bf{X})^{-1}\bf{X}^{\rm T}\boldsymbol y
$$
令 $\Large \hat{\boldsymbol{x}}_i = (\boldsymbol x_i;1)$，则线性回归模型为：
$$
\Large
f(\hat{\boldsymbol{x}}_i) = \hat{\boldsymbol{x}}_i^{\rm T} (\bf{X}^{\rm T}\bf{X})^{-1}\bf{X}^{\rm T}\boldsymbol y
$$

---

实际情况中，样本属性数比样本数多的情况也存在，$\Large \bf{X}$ 不一定满秩（即  $\Large \bf{X}^{\rm T}\bf{X}$ 不一定正定），此时式(13)线性方程组存在多个解，这时就需要学习算法的偏好来决定了。

---

实际情况中，对式（2）的左侧做非线性变换，就可以得到广义的线性模型：
$$
\Large 
f(\boldsymbol x) = g^{-1}(\boldsymbol w^{\rm T} \boldsymbol x + b)\quad ,g是单调可微函数
$$
如：
$$
\Large
\begin{align}\begin{split}
{\rm {ln}}f(\boldsymbol x) &= \boldsymbol w^{\rm T} \boldsymbol x + b \\
f(\boldsymbol x) &= e^{w^{\rm T}\boldsymbol x + b}
\end{split}\end{align}
$$

## 3.3 对数几率回归

对于回归任务来说，利用广义线性模型即可得到预测值；对于分类任务而言，真实结果是离散的，理想的联系函数$\Large g$ 为”单位阶跃函数“，如：
$$
\Large
y = 
\begin{cases}
0,\quad & z \lt 0;\\
0.5 ,\quad & z = 0;\\
1, \quad & z \gt 0;
\end{cases}
$$
但是”单位阶跃函数“在阶跃的位置不连续，不符合广义线性模型的定义；因此，我们找到了”**对数几率函数**“来近似模拟”单位阶跃函数“:
$$
\Large
\begin{align}\
y = \cfrac{1}{1+e^{-z}} & = \cfrac{1}{1 + e^{-(\boldsymbol w^{\rm T}\boldsymbol x + b)}} \\
\rm{ln}\cfrac{y}{1-y} & = \boldsymbol w^{\rm T}\boldsymbol x + b
\end{align}
$$
将式（20）称为对数几率，是样本 $\Large \boldsymbol x$ 为正例的可能性的对数。

> 其函数图像如下：
>
> ![image-20210705143533760](https://cdn.astero.xyz/img/image-20210705143533760.png)

使用**最大似然法**求解 $\Large \boldsymbol w$ 和 $\Large b$ ，则有**对数似然**：
$$
\Large
\ell(\boldsymbol w,b) = \sum^m_{i=1} {\rm{ln}}\;p(y_i\;|\;\boldsymbol x_i;\boldsymbol w,b)
$$
其中：
$$
\Large
p(y_i\;|\;\boldsymbol x_i;\boldsymbol w,b)
$$
表示 “已知样本为 $\Large \boldsymbol x_i$ 时，得出预测结果为 $\Large y_i$ 的后验概率”。 问题转化为求 $\Large \boldsymbol \beta$ 使得：
$$
\Large
\ell(\boldsymbol \beta) = \sum^m_{i=1}(-y_i\boldsymbol \beta^{\rm T}\hat{\boldsymbol x}_i + {\rm ln}(1+e^{\boldsymbol \beta ^{\rm T}\hat{\boldsymbol x}_i})),\quad \boldsymbol \beta = (\boldsymbol w;b)
$$
取最小值。由于 $\Large \ell(\boldsymbol \beta)$ 是高阶可导连续凸函数，根据凸优化理论，可以使用经典数值优化算法如梯度下降法，牛顿法等进行求解。（过程略）

> 关于先验概率、后验概率、贝叶斯公式最大似然法参考 [详解最大似然估计（MLE）、最大后验概率估计（MAP），以及贝叶斯公式的理解](https://blog.csdn.net/u011508640/article/details/72815981)，以及[如何通俗地理解概率论中的「极大似然估计法」?](https://blog.csdn.net/weixin_42159940/article/details/91077943)、[贝叶斯公式的理解（先验概率/后验概率）](https://blog.csdn.net/m0_38052384/article/details/93368629)

## 3.4 线性判别分析（LDA）

线性判别分析全称Linear Discriminant Analysis，基本思想是将**高维**的模式样本投影到最佳鉴别矢量空间，以达到**抽取分类信息**和**压缩特征空间维数**的效果，投影后保证模式样本在新的子空间有**最大的类间距离**和**最小的类内距离**，即模式在该空间中有最佳的可分离性。因此，它是一种有效的特征抽取方法。使用这种方法能够使投影后模式样本的类间散布矩阵最大，并且同时类内散布矩阵最小。就是说，它能够保证投影后模式样本在新的空间中有最小的类内距离和最大的类间距离，即模式在该空间中有最佳的可分离性。

> 是一种降维方法；类似于主成分分析（PCA），都基于特征值与特性向量实现降维处理；
>
> 但是
>
> - PCA中是没有引入分类的，属于无监督学习；
> - LDA的结果是将数据投影到不同分类、PCA的结果是将数据投影到最高相似分组；
> - PCA倾向于将特征省略或合并，有可能带来精度损失；
>
> 因此，通常情况下，LDA是PCA的补充，如果PCA的模型对一些相近特征进行了合并处理的话，LDA可以较好地进行分离。

基本思想是：

<center><span style="color:green">最大化类间方差与最小化类内方差，即减少分类内部之间的差异，而扩大不同分类之间的差异</span></center>

如图所示，对于二分类问题来说，就是找到一条直线，使得所有样本在线上的投影满足上述的思想。

![img](https://cdn.astero.xyz/img/v2-cf8377ffac0059e6411ab5ae3c7419a3_720w.jpg)

> 在上图我们可以看到两条曲线，LD1和LD2，和数据点分别在他们的投影，我们可以比较在LD1和LD2的投影，**在LD1的投影两个类间距较大，而在LD2的投影，间距很小，几乎快要重合，同时我们也可以发现，在LD1上的投影密度也较LD2更大**，这就满足了最大化类间方差，和最小化类内方差。

数学定义如下：

---

给定数据集：$\Large D = \{ (\boldsymbol x_i, y_i)\}^m_{i=1}$ ；令 $\Large X_i, \boldsymbol \mu_i, \boldsymbol \Sigma_i$ 分别表示：

“**第** $\Large i \in N$ **类示例的集合、均值向量，协方差矩阵**” ；令 $\Large \boldsymbol w$ 为待投影的直线（这里直线可以用向量进行表示，后面会看到计算最值时是和 $\Large \boldsymbol w$ 的长度无关的），即：
$$
\Large
\begin{align}
\boldsymbol \mu_i &= \cfrac{1}{|X_i|}\sum_{x_i \in X_i}\boldsymbol x_i \\
\boldsymbol \Sigma_i &= \sum_{y_i \in X_i}(y_i - \bar{\boldsymbol\mu}_i)^2
\end{align}
$$


将数据进行投影可得：

- 每个分类的投影的中心为：$\Large \bar{\boldsymbol\mu}_i = \boldsymbol w^{\rm T}\boldsymbol \mu_i$ （因为 $\Large \boldsymbol \mu_i$ 是该分类的均值向量）；
- 每个分类的投影的协方差为：$\Large \boldsymbol w^{\rm T}{\boldsymbol \Sigma_i}\boldsymbol w$ （从 $\Large \boldsymbol \mu_i$ 的定义可得）；

讨论二分类问题，根据中心思想：

- 同类型样本的投影协方差尽可能小，即 $\Large \boldsymbol w^{\rm T}{\boldsymbol \Sigma_0}\boldsymbol w + \boldsymbol w^{\rm T}{\boldsymbol \Sigma_1}\boldsymbol w$ 尽可能小;
- 不同类型的投影中心距离尽可能大，即 $\Large ||\; \boldsymbol w^{\rm T}\boldsymbol \mu_0 - \boldsymbol w^{\rm T}\boldsymbol \mu_1 \;||^2_2$ 尽可能大；

同时考虑两者，定义欲最大化的目标为（二分类时）：
$$
\Large
\begin{align}\begin{split}
J &= \cfrac
{||\; \boldsymbol w^{\rm T}\boldsymbol \mu_0 - \boldsymbol w^{\rm T}\boldsymbol \mu_1 \;||^2_2}
{\boldsymbol w^{\rm T}{\boldsymbol \Sigma_0}\boldsymbol w + \boldsymbol w^{\rm T}{\boldsymbol \Sigma_1}\boldsymbol w} \\

&= \cfrac
{\boldsymbol w^{\rm T} (\boldsymbol \mu_0 - \boldsymbol \mu_1) (\boldsymbol \mu_0 - \boldsymbol \mu_1)^{\rm T} \boldsymbol w}
{\boldsymbol w^{\rm T} (\boldsymbol \Sigma_o+ \boldsymbol \Sigma_1) \boldsymbol w}
\end{split}\end{align}
$$
定义**类内散度矩阵（within-class scatter matrix）**：
$$
\Large 
\begin{align}\begin{split}

\boldsymbol {\rm S}_w &= \sum_{i=1}^N \boldsymbol {\rm S}_i \\
&= \sum_{i=1}^N \sum_{x_k \in X_i} (\boldsymbol \mu_i - \boldsymbol x_k)(\boldsymbol \mu_i - \boldsymbol x_k)^{\rm T}
\end{split}\end{align}
$$
总体样本均值为：
$$
\Large
\boldsymbol \mu = \cfrac{1}{m}\sum^m_{i = 1}\boldsymbol x_i
$$


**类间散度矩阵（between-class scatter matrix）**：
$$
\Large
\begin{align}\begin{split}
\boldsymbol {\rm {S}}_{b_{i,j}} &= (\boldsymbol \mu_i - \boldsymbol \mu_j)(\boldsymbol \mu_i - \boldsymbol \mu_j)^{\rm T} 
\end{split}\end{align}
$$
故式（26）可写为 $\Large \boldsymbol {\rm S}_b$ 和 $\Large \boldsymbol {\rm S}_w$ 的**广义瑞利商**的形式：
$$
\Large
J = \cfrac{\boldsymbol w^{\rm T} \boldsymbol {\rm S}_b \boldsymbol w}{\boldsymbol w^{\rm T} \boldsymbol {\rm S}_w \boldsymbol w}
$$
**全局散列矩阵**为：
$$
\Large
\begin{align}\begin{split}
\boldsymbol {\rm S}_t &= \boldsymbol {\rm S}_b + \boldsymbol {\rm S}_w \\
&= \sum^m_{i=1}(\boldsymbol x_i - \boldsymbol\mu)(\boldsymbol x_i - \boldsymbol \mu)^{\rm T}
\end{split}\end{align}
$$
则：
$$
\Large
\begin{align}\begin{split}
\boldsymbol {\rm S}_b &= \boldsymbol {\rm S}_t - \boldsymbol {\rm S}_w \\
&= \sum^N_{i=1}m_i(\boldsymbol \mu_i - \boldsymbol\mu)(\boldsymbol \mu_i - \boldsymbol \mu)^{\rm T}
\end{split}\end{align}
$$
综合 （27），（31），（32），$\Large \boldsymbol {\rm S}_b, \boldsymbol {\rm S}_t, \boldsymbol {\rm S}_w$ 中取任意两个都可对 $\Large \boldsymbol w $ 进行求解，常用的是优化目标：
$$
\Large
{\rm max_W}
$$


> 特别的，在二分类中可以使用拉格朗日算子求解： 
> $$
> \Large 
> \boldsymbol {\rm S}_b \boldsymbol w = \lambda \boldsymbol {\rm S}_w \boldsymbol w
> $$
> 因为 $\Large \boldsymbol {\rm S}_b \boldsymbol w$ 的方向恒为 $\Large \boldsymbol \mu_0 - \boldsymbol \mu_1$ ，令 $\Large \boldsymbol {\rm S}_b \boldsymbol w = \lambda (\boldsymbol \mu_0 - \boldsymbol \mu_1)$  ，代入（33）中有：
> $$
> \Large
> \boldsymbol w = \boldsymbol {\rm S}_w^{-1}(\boldsymbol \mu_0 - \boldsymbol \mu_1)
> $$

> LDA可以从贝叶斯决策理论的角度进行解释，可证：当
>
> - 两类数据同先验；
> - 满足高斯分布且协方差相等；
>
> 时，LDA可以达到最优分类；

## 3.5 多分类问题

由于一次性求解多分类问题较困难，现实中的多分类问题通常是将多分类**拆解为若干个二分类问题**来求解。拆解策略有：

- OvO：一对一；
- OvR：一对其余；
- MvM：多对多；

## 3.6 类别不平衡问题

## 公式推导

### 推导（6）：

已知$\Large E_{(w, b)}=\sum\limits_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2}$，所以 
$$
\Large
\begin{align}\begin{split} 
\cfrac{\partial E_{(w, b)}}{\partial w}&=\cfrac{\partial}{\partial w} \left[\sum_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2}\right] \\ 
&= \sum_{i=1}^{m}\cfrac{\partial}{\partial w} \left[\left(y_{i}-w x_{i}-b\right)^{2}\right] \\ 
&= \sum_{i=1}^{m}\left[2\cdot\left(y_{i}-w x_{i}-b\right)\cdot (-x_i)\right] \\ 
&= \sum_{i=1}^{m}\left[2\cdot\left(w x_{i}^2-y_i x_i +bx_i\right)\right] \\ 
&= 2\cdot\left(w\sum_{i=1}^{m} x_{i}^2-\sum_{i=1}^{m}y_i x_i +b\sum_{i=1}^{m}x_i\right) \\ 
&=2\left(w \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}\right) \end{split}\end{align}
$$

### 推导（7）：

已知$\Large E_{(w, b)}=\sum\limits_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2}$，所以 
$$
\Large
\begin{align}\begin{split} 
\cfrac{\partial E_{(w, b)}}{\partial b}
&=\cfrac{\partial}{\partial b} \left[\sum_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2}\right] \\ 
&=\sum_{i=1}^{m}\cfrac{\partial}{\partial b} \left[\left(y_{i}-w x_{i}-b\right)^{2}\right] \\ 
&=\sum_{i=1}^{m}\left[2\cdot\left(y_{i}-w x_{i}-b\right)\cdot (-1)\right] \\
&=\sum_{i=1}^{m}\left[2\cdot\left(b-y_{i}+w x_{i}\right)\right] \\
&=2\cdot\left[\sum_{i=1}^{m}b-\sum_{i=1}^{m}y_{i}+\sum_{i=1}^{m}w x_{i}\right] \\
&=2\left(m b-\sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)\right) 
\end{split}\end{align}
$$

### 推导（8）：

令公式(6)等于0 
$$
\Large 0 = w\sum_{i=1}^{m}x_i^2-\sum_{i=1}^{m}(y_i-b)x_i
$$
$$
\Large w\sum_{i=1}^{m}x_i^2 = \sum_{i=1}^{m}y_ix_i-\sum_{i=1}^{m}bx_i
$$
由于令公式(7)等于0可得$\Large b=\cfrac{1}{m}\sum_{i=1}^{m}(y_i-wx_i)$，又因为$\Large \cfrac{1}{m}\sum_{i=1}^{m}y_i=\bar{y}$，$\Large \cfrac{1}{m}\sum_{i=1}^{m}x_i=\bar{x}$，则$\Large b=\bar{y}-w\bar{x}$，代入上式可得
$$
\Large \begin{align}\begin{split} w\sum_{i=1}^{m}x_i^2 & = \sum_{i=1}^{m}y_ix_i-\sum_{i=1}^{m}(\bar{y}-w\bar{x})x_i \\
w\sum_{i=1}^{m}x_i^2 & = \sum_{i=1}^{m}y_ix_i-\bar{y}\sum_{i=1}^{m}x_i+w\bar{x}\sum_{i=1}^{m}x_i \\
w(\sum_{i=1}^{m}x_i^2-\bar{x}\sum_{i=1}^{m}x_i) & = \sum_{i=1}^{m}y_ix_i-\bar{y}\sum_{i=1}^{m}x_i \\
w & = \cfrac{\sum_{i=1}^{m}y_ix_i-\bar{y}\sum_{i=1}^{m}x_i}{\sum_{i=1}^{m}x_i^2-\bar{x}\sum_{i=1}^{m}x_i} \end{split}\end{align}
$$
由于$\Large\bar{y}\sum_{i=1}^{m}x_i=\cfrac{1}{m}\sum_{i=1}^{m}y_i\sum_{i=1}^{m}x_i=\bar{x}\sum_{i=1}^{m}y_i$，$\Large\bar{x}\sum_{i=1}^{m}x_i=\cfrac{1}{m}\sum_{i=1}^{m}x_i\sum_{i=1}^{m}x_i=\cfrac{1}{m}(\sum_{i=1}^{m}x_i)^2$，代入上式即可得公式(8) 
$$
\Large w=\cfrac{\sum_{i=1}^{m}y_i(x_i-\bar{x})}{\sum_{i=1}^{m}x_i^2-\cfrac{1}{m}(\sum_{i=1}^{m}x_i)^2}
$$
如果要想用Python来实现上式的话，上式中的求和运算只能用循环来实现，但是如果我们能将上式给向量化，也就是转换成矩阵（向量）运算的话，那么我们就可以利用诸如NumPy这种专门加速矩阵运算的类库来进行编写。下面我们就尝试将上式进行向量化，将$\Large \cfrac{1}{m}(\sum_{i=1}^{m}x_i)^2=\bar{x}\sum_{i=1}^{m}x_i $代入分母可得 
$$
\Large
\begin{align}
w & = \cfrac{\sum_{i=1}^{m}y_i(x_i-\bar{x})}{\sum_{i=1}^{m}x_i^2-\bar{x}\sum_{i=1}^{m}x_i} \\ & = \cfrac{\sum_{i=1}^{m}(y_ix_i-y_i\bar{x})}{\sum_{i=1}^{m}(x_i^2-x_i\bar{x})} \end{align}
$$
又因为$\Large\bar{y}\sum_{i=1}^{m}x_i=\bar{x}\sum_{i=1}^{m}y_i=\sum_{i=1}^{m}\bar{y}x_i=\sum_{i=1}^{m}\bar{x}y_i=m\bar{x}\bar{y}=\sum_{i=1}^{m}\bar{x}\bar{y}$，$\Large\sum_{i=1}^{m}x_i\bar{x}=\bar{x}\sum_{i=1}^{m}x_i=\bar{x}\cdot m \cdot\frac{1}{m}\cdot\sum_{i=1}^{m}x_i=m\bar{x}^2=\sum_{i=1}^{m}\bar{x}^2$，则上式可化为 
$$
\Large\begin{align} w & = \cfrac{\sum_{i=1}^{m}(y_ix_i-y_i\bar{x}-x_i\bar{y}+\bar{x}\bar{y})}{\sum_{i=1}^{m}(x_i^2-x_i\bar{x}-x_i\bar{x}+\bar{x}^2)} \\ & = \cfrac{\sum_{i=1}^{m}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{m}(x_i-\bar{x})^2} \end{align}
$$
若令$\Large\boldsymbol{x}=(x_1,x_2,...,x_m)^T$，$\Large\boldsymbol{x}_{d}=(x_1-\bar{x},x_2-\bar{x},...,x_m-\bar{x})^T$为去均值后的$\Large\boldsymbol{x}$，$\Large\boldsymbol{y}=(y_1,y_2,...,y_m)^T$，$\Large\boldsymbol{y}_{d}=(y_1-\bar{y},y_2-\bar{y},...,y_m-\bar{y})^T$为去均值后的$\Large\boldsymbol{y}$，其中$\Large\boldsymbol{x}$、$\Large\boldsymbol{x}_{d}$、$\Large\boldsymbol{y}$、$\Large\boldsymbol{y}_{d}$均为m行1列的列向量，代入上式可得
$$
\Large w=\cfrac{\boldsymbol{x}_{d}^T\boldsymbol{y}_{d}}{\boldsymbol{x}_d^T\boldsymbol{x}_{d}}
$$

### 推导（23）

令 $\Large \boldsymbol \beta = (\boldsymbol w;b)$ ，$\Large \hat{\boldsymbol x} = (\boldsymbol x;1)$ ，则 $\Large \boldsymbol w^{\rm T}\boldsymbol x  + b = \boldsymbol \beta^{\rm T} \hat{\boldsymbol x}$ ；

再令 $\Large p_1(\hat{\boldsymbol x};\boldsymbol \beta) = p(y=1|\hat{\boldsymbol x};\boldsymbol \beta)$ ，$\Large p_0(\hat{\boldsymbol x};\boldsymbol \beta) = p(y=0|\hat{\boldsymbol x};\boldsymbol \beta) = 1 - p_1(\hat{\boldsymbol x};\boldsymbol \beta)$ ；

似然项可重写为：
$$
\Large 
p(y_i|\boldsymbol x_i;\boldsymbol w,b) = y_ip_1(\hat{\boldsymbol x}_i;\boldsymbol \beta) + (1-y_i)p_0(\hat{\boldsymbol x}_i;\boldsymbol \beta)
$$
代入式（21）可得：
$$
\Large
\ell(\boldsymbol{\beta})=\sum_{i=1}^{m}\ln\left(y_ip_1(\hat{\boldsymbol x}_i;\boldsymbol{\beta})+(1-y_i)p_0(\hat{\boldsymbol x}_i;\boldsymbol{\beta})\right)
$$

其中$\Large p_1(\hat{\boldsymbol x}_i;\boldsymbol{\beta})=\cfrac{e^{\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i}}{1+e^{\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i}},p_0(\hat{\boldsymbol x}_i;\boldsymbol{\beta})=\cfrac{1}{1+e^{\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i}} $，带入上式可得：
$$
\Large
\begin{align}\begin{split} \ell(\boldsymbol{\beta})&=\sum_{i=1}^{m}\ln\left(\cfrac{y_ie^{\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i}+1-y_i}{1+e^{\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}i}}\right) \\ &=\sum_{i=1}^{m}\left(\ln(y_ie^{\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i}+1-y_i)-\ln(1+e^{\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i})\right) 
\end{split}\end{align}
$$

由于$\Large y_i $= $\Large 0$ 或 $\Large 1$ ，则 
$$
\Large
\ell(\boldsymbol{\beta}) = 
\begin{cases} \sum_{i=1}^{m}(-\ln(1+e^{\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i})), & y_i=0 \\ \sum_{i=1}^{m}(\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i-\ln(1+e^{\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i})), & y_i=1 
\end{cases}
$$

两式综合可得 
$$
\Large
\ell(\boldsymbol{\beta})=\sum_{i=1}^{m}\left(y_i\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i-\ln(1+e^{\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i})\right)
$$

由于此式仍为极大似然估计的似然函数，所以最大化似然函数等价于最小化似然函数的相反数，也即在似然函数前添加负号即可得公式（23）。

值得一提的是，若将公式（35）这个似然项改写为$\Large p(y_i|\boldsymbol x_i;\boldsymbol w,b)=[p_1(\hat{\boldsymbol x}_i;\boldsymbol{\beta})]^{y_i}[p_0(\hat{\boldsymbol x}_i;\boldsymbol{\beta})]^{1-y_i}$，再将其代入公式（21）可得 
$$
\Large
\begin{align} \begin{split}
\ell(\boldsymbol{\beta}) &= \sum_{i=1}^{m} \ln([p_1(\hat{\boldsymbol x}_i;\boldsymbol{\beta})]^{y_i}[p_0(\hat{\boldsymbol x}_i;\boldsymbol{\beta})]^{1-y_i}) \\

&=\sum_{i=1}^{m}[y_i\ln(p_1(\hat{\boldsymbol x}_i;\boldsymbol{\beta}))+(1-y_i)\ln(p_0(\hat{\boldsymbol x}_i;\boldsymbol{\beta}))] \\ 

&=\sum_{i=1}^{m} \{ y_i[\ln(p_1(\hat{\boldsymbol x}_i;\boldsymbol{\beta}))-\ln(p_0(\hat{\boldsymbol x}_i;\boldsymbol{\beta}))]+\ln(p_0(\hat{\boldsymbol x}_i;\boldsymbol{\beta}))\} \\

&=\sum_{i=1}^{m}\left[y_i\ln\left(\cfrac{p_1(\hat{\boldsymbol x}_i;\boldsymbol{\beta})}{p_0(\hat{\boldsymbol x}_i;\boldsymbol{\beta})}\right)+\ln\left(p_0(\hat{\boldsymbol x}_i;\boldsymbol{\beta})\right)\right] \\

&=\sum_{i=1}^{m}\left[y_i\ln\left(e^{\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i}\right)+\ln\left(\cfrac{1}{1+e^{\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i}}\right)\right] \\ 

&=\sum_{i=1}^{m}\left(y_i\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i-\ln(1+e^{\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i})\right) 
\end{split} \end{align}
$$
显然，此种方式更易推导出公式（23）。