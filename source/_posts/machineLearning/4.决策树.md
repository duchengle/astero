---
title: 4. 决策树
date: 2021-06-17
categories: 
  - MachineLearning-周志华
tags:
  - readnotes
  - ml
---

## 4.1 基本流程

也称判定树。

- 根节点：包含所有测试属性的样本集合；
- 子节点：属性测试，包含属性划分后的样本集合；
- 叶子节点：决策结果；

构建决策树的过程是递归地根据属性值和结果构建树的过程。

递归操作是：

- 从当前节点包含的样本未划分的属性中选择一个属性进行划分，创建其子节点；

递归的退出条件有：

1. 当前节点中的样本全部属于同一类别，即结果相同，则当前节点为叶子节点；
2. 不满足条件1，且当前节点中的样本所有属性值一致，无法进行划分；此时一般取节点中样本的多数类别作为结果；当前节点为叶子节点；
3. 不满足1，2；当前划分的节点中无样本，无法处理，当前节点剪除；

## 4.2 划分选择

决策树的构建过程即是将无序的样本数据有序化的过程，引入“纯度”和“熵”的概念进行处理。

自然我们希望决策树的复杂度越低越好，因此每个节点中的数据“纯度”应该越高越好，这样需要做的属性划分就少。

### 4.2.1 信息增益

定义”信息熵“来表示节点的“纯度”，是节点总样本的所有类别的概率与其对数的乘机之和的相反数；
$$
\Large
{\rm Ent}(D) = - \sum^{|\gamma|}_{k = 1}p_k\log_2p_k, \quad p_k表示D中第k类样本的比例
$$
信息熵越小，数据纯度越高。

定义信息增益为：**属性划分后的信息熵与划分前的信息熵之差**的相反数。则增益越大，划分后数据提升的纯度越高。故在寻找进行划分的属性时遍历所有待划分属性，分别计算其信息增益，取其最高者进行划分。
$$
\Large
{\rm Gain}(D,a) = {\rm Ent}(D,a) - \sum^V_{v=1}\cfrac{|D^v|}{|D|}{\rm Ent}(D^v)
$$
代表算法：ID3

### 4.2.2 增益率

可以直观的发现（以及证明），使用信息增益进行划分时，由于定义的原因，**选择的属性总是偏向于该属性取值的可能较多的那些属性**。为避免这种现象，引入“代价”的概念，将信息增益除以其划分属性的“**固有值**”，惩罚取值可能较多的属性。
$$
\Large
\begin{align}
{\rm Gain.ratio}(D, a) &= \cfrac{{\rm Gain}(D,a)}{{\rm IV}(a)} \\
{\rm IV}(a) &= -\sum_{v=1}^{V} \cfrac{|D^v|}{|D|}\log_2\cfrac{|D^v|}{|D|} 
\end{align}
$$


这样一来使用“增益率”又会偏好取值可能较少的属性，因此，通常是综合信息增益与增益率：

- 首先选取信息增益高出平均值的属性；
- 然后选择其中增益率最高的。

即每次选取的都是增益和增益率较为平衡的。

代表算法：C4.5

### 4.2.3 基尼指数

基尼指数反映了从数据集中随机取两个样本，他们的类别标记不一致的概率：
$$
\Large
\begin{align}
{\rm Gini}(D) &= \sum^{|\gamma|}_{k=1}\sum_{k' \neq k}p_kp_{k'}= 1-\sum_{k=1}^{|\gamma|}p_k^2 \\
{\rm Gini\_index}(D,a) &= \sum^V_{v=1}\cfrac{|D^v|}{|D|}{\rm Gini}(D^v)
\end{align}
$$
每次选取基尼指数最小的属性进行划分。

代表算法：CART

## 4.3剪枝处理

完全按照样本集严格构建的决策树容易陷入过拟合，因此需要人为地去掉一些分支，即剪枝。可分为：

- **预剪枝（pre-pruning）**：在决策树生成过程中，对节点进行划分时进行判断，对比划分前与划分后的泛化性能，决定是否还需要进行划分；
- **后剪枝（post-pruning）**：决策时生成完成后，自底向上考察子节点，以“将其子树换成叶子节点是否能提升泛化能力”作为考察目标。

剪枝的依据就是划分前后，决策树的泛化性能是否有提升，如没有提升，则不进行划分。

易知：

- 预剪枝为避免过拟合，使用类“贪心”的模式进行剪枝，在样本不足时容易欠拟合；
- 后剪枝的欠拟合风险较小，但是由于训练时没有剪枝，导致训练和剪枝开销要大得多。

## 4.4连续与缺失值

### 4.4.1 连续值处理

定义一系列可以划分连续值的“**划分点**”，使用计算信息增益的方法来确定使用哪个划分点。

与离散属性划分不同，连续属性可以在子树中多次划分，因此每次划分时都要进行计算。

### 4.4.2 缺失值处理

## 4.5 多变量决策树

